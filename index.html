<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Muhammad Naeem</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Muhammad Naeem</h1>
</div>
<table class="imgtable"><tr><td>
<img src="./Figures/photo4.png" alt="Photo" width="250px" height="250px" />&nbsp;</td>
<td align="left"><p>Muhammad Naeem  <br /> 
Graduate Student in <a href="http://www.lmars.whu.edu.cn/en/">LIESMARS</a><br />
Wuhan University, Hubei, China <br />
Advisor: <a href="http://l.web.umkc.edu/lizhu/">Prof. Xiongwu Xiao</a><br /></p>
<p>
<b>Email:</b> m.naeem4288@gmail.com<br />
<b>LinkedIn:</b> <a href="https://www.linkedin.com/in/muhammadnaeem27">linkedin.com/in/muhammadnaeem27</a><br />
<b>Portfolio:</b> <a href="https://muhammadnaeem27.github.io/">muhammadnaeem27.github.io</a>
</p>
<p><a href="https://scholar.google.com/citations?hl=en&amp;user=hL-LaAQAAAAJ&amp;sortby=pubdate">Google Scholar Link</a> <br />
<a href="https://www.linkedin.com/in/aniqueakhtar/">LinkedIn</a></p>
</td></tr></table>
<p><br />
Researcher with experience in deep learning, computer vision, and image generation spanning both industry and academia. Currently I'm pursuing my Master's in Photogrammetry and Remote Sensing from Wuhan University - China. I am working under the supervision of <a href="https://www.researchgate.net/profile/Xiongwu-Xiao">Prof. Xiongwu Xiao</a> in the State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing <a href="http://www.lmars.whu.edu.cn/en/">(LIESMARS)</a>.<br /><br />
Previously, I worked as a Senior AI Developer and Team Lead at <a href="https://www.octaloop.io/">Octaloop Technologies</a>, leading the Computer Vision Team in projects like Virtual Room Staging, Virtual Try-on, and Video Game Streaming Highlights Generation, ensuring seamless AI integration and mentoring team members.<br /><br />
I did my B.Sc in Computer Science (major in AI) from <a href="http://www.ku.edu.tr/">COMSATS University</a>, Islamabad, Pakistan,
where I worked under <a href="https://scholar.google.com/citations?user=96XFWaUAAAAJ&hl=en">Dr. Jamal Hussain Shah.</a><br /><br />
<br /></p>
<td align="left">

</td></tr></table>

<div class="infoblock">
<div class="blockcontent">
<h2>Education</h2>
<ul>
<li>
<b>COMSATS University Islamabad, Pakistan</b> <br />
Bachelor of Science in Computer Science <br />
September 2019 – June 2023 <br />
Relevant Coursework: Data Structures and Algorithm, Digital Image Processing, Machine Learning, Computer Vision, Pattern Recognition
</li>
</ul>
</div>
</div>

<!-- <div class="infoblock">
<div class="blockcontent">
<h2>Skills</h2>
<p>
Python, C++, HTML/CSS, NumPy, Pandas, OpenCV, PyTorch, TensorFlow, Keras, Git, AWS, Fast API
</p>
</div>
</div> -->

<div class="infoblock">
<div class="blockcontent">
<h2>Experience</h2>
<ul>
<li>
<b>Octaloop Technologies</b> <br />
Senior AI Developer / Team Lead, Computer Vision Team <br />
April 2024 – Aug 2025
<ul>
<li>Lead and oversee the Computer Vision Team, developing advanced solutions for projects such as Virtual Room Staging, Virtual Try-on, and Video Game Streaming Highlights Generation using scene analysis.</li>
<li>Supervise and mentor team members, ensuring seamless integration of AI functionalities into client applications and fostering a collaborative environment to drive innovation.</li>
<li>Stay updated with the latest advancements in AI and computer vision, applying cutting-edge techniques to enhance project performance and client satisfaction.</li>
<li><b>Tools:</b> Python, PyTorch, TensorFlow, YOLO-v5, OpenCV, Keras, Fast API, AWS, GCP</li>
</ul>
</li>
<li>
<b>NineSol Technologies</b> <br />
AI Developer <br />
April 2023 – April 2024
<ul>
<li>Developed and integrated the latest models into the backend of mobile and web applications, enhancing functionality with features such as Background Removal, Colorization, Document Scanning, Virtual Try-on, and Transcription.</li>
<li>Stayed informed with the latest industry trends and emerging technologies, applying this knowledge to refine and advance features within diverse applications.</li>
<li>Collaborated with other team members to smoothly embed advanced functionalities, ensuring seamless user experiences in mobile applications.</li>
<li><b>Tools:</b> Python, PyTorch, TensorFlow, YOLO-v5, OpenCV, Keras, Fast API, AWS, GCP</li>
</ul>
</li>
</ul>
</div>
</div>

<!-- <div class="infoblock">
<div class="blockcontent"> --> 


<h2>Projects</h2>

</td></tr></table>

<!-- Virtual Staging -->
<table class="imgtable"><tr>
<td style="width:180px; vertical-align:top;">
  <img src="./Figures/virtual_staging.gif" alt="Project GIF" width="500px" height="300px" />
</td>
<td align="left">
  <b>Virtual Staging</b><br />
  <i>Python, PyTorch, Stable Diffusion, ControlNet, OpenCV, NumPy, Scikit-Learn</i>
  <ul>
    <li>Implemented an inpainting approach using Stable Diffusion with ControlNet to furnish empty room images without altering existing items and color combinations.</li>
    <li>Applied semantic segmentation on empty rooms to identify areas and items to preserve, generating binary images from segmented masks.</li>
    <li>Used tailored prompts for bedrooms and living rooms, passing binary masked images, prompts, and empty room images to the Stable Diffusion inpainting model to generate furniture in the specified areas.</li>
  </ul>
</td>
</tr></table>

<!-- Video-games Streaming Highlight Generation -->
<table class="imgtable"><tr>
<td style="width:180px; vertical-align:top;">
  <img src="./Figures/highlights.gif" alt="Project GIF" width="500px" height="300px" />
</td>
<td align="left">
  <b>Video-games Streaming Highlight Generation</b><br />
  <i>Python, PyTorch, OpenCV, NLTK, Scikit-Learn</i>
  <ul>
    <li>Developed a system to generate interesting highlights from 6-7 hours of video game streaming videos using three main approaches: Sound, Transcription, and Movement.</li>
    <li>Implemented separate AI techniques for each approach to identify key timestamps, leveraging audio analysis, natural language processing, and computer vision.</li>
    <li>Combined results from all three approaches in a specific sequence to produce the best highlights and generate a final video.</li>
  </ul>
</td>
</tr></table>

<!-- Weed Detection Robotic Car -->
<table class="imgtable"><tr>
<td style="width:180px; vertical-align:top;">
  <img src="./Figures/weed.gif" alt="Project GIF" width="500px" height="300px" />
</td>
<td align="left">
  <b>Weed Detection Robotic Car</b><br />
  <i>YOLO-v5, TensorFlow, Flask, Tkinter, VS code, Colab, Roboflow, labelimg</i>
  <ul>
    <li>Compiled a diverse dataset comprising four categories of weed. Implemented YOLO-v5 for real-time object detection, achieving precise identification of weeds in agricultural fields.</li>
    <li>Designed and built a robotic car prototype that uses a smartphone camera and DC motors to wirelessly stream live images to the YOLO-v5 model via FastAPI.</li>
    <li>Integrated an Arduino-controlled LED system with a matrix-based system to precisely locate and eliminate detected weeds.</li>
    <li>Created a website to showcase project insights, allowing users to engage with the trained YOLO-v5 model and evaluate its effectiveness.</li>
    <li>Used data visualization to highlight the project's technological innovation and potential for sustainable agricultural practices.</li>
  </ul>
</td>
</tr></table>

<!-- Portrait Background Remover -->
<table class="imgtable"><tr>
<td style="width:180px; vertical-align:top;">
  <img src="./Figures/bg.gif" alt="Project GIF" width="500px" height="300px" />
</td>
<td align="left">
  <b>Portrait Background Remover</b><br />
  <i>Python, PyTorch, U-Net, NumPy, Scikit-Learn, OpenCV</i>
  <ul>
    <li>Used U-Net structures for both rough and detailed portrait segmentation, addressing the problem of subject isolation in images.</li>
    <li>Utilized internet and publicly available datasets like COCO for training data.</li>
  </ul>
</td>
</tr></table>

<!-- Colorize Gray scale Image -->
<table class="imgtable"><tr>
<td style="width:180px; vertical-align:top;">
  <img src="./Figures/colorize.gif" alt="Project GIF" width="500px" height="300px" />
</td>
<td align="left">
  <b>Colorize Gray scale Image</b><br />
  <i>Python, PyTorch, U-Net, Fast API, Colab, VS code</i>
  <ul>
    <li>Included two time-scale update rules, self-attention GANs, and inflection points for effective and error-free coloring.</li>
    <li>Used U-Net architecture with model-specific backbone choices (resnet34/resnet101), spectral normalization, and self-attention.</li>
    <li>Implemented Perceptual Loss based on VGG16 during NoGAN learning for realistic colorization.</li>
  </ul>
</td>
</tr></table>

<!-- Image to Talking Portrait -->
<table class="imgtable"><tr>
<td style="width:180px; vertical-align:top;">
  <img src="./Figures/talkingimg.gif" alt="Project GIF" width="500px" height="300px" />
</td>
<td align="left">
  <b>Image to Talking Portrait</b><br />
  <i>Python, Dlib, face-recognition, PyTorch, ffmpeg, OpenCV, Fast API, Colab, VS code</i>
  <ul>
    <li>Created talking videos by combining lips and head movements of a reference video with a source image using Dlib for facial landmark identification.</li>
    <li>Used Wav2Lip-hq for precise lip movement synchronization and ESRGAN for video up-sampling.</li>
    <li>Integrated synchronized audio and used ffmpeg and OpenCV for efficient video processing.</li>
  </ul>
</td>
</tr></table>

<!-- Document Scanner -->
<table class="imgtable"><tr>
<td style="width:180px; vertical-align:top;">
  <img src="./Figures/documentscanner.gif" alt="Project GIF" width="500px" height="300px" />
</td>
<td align="left">
  <b>Document Scanner</b><br />
  <i>Python, OpenCV, PyTorch, Fast API, Colab, VS code</i>
  <ul>
    <li>Created a Document Scanner with a Geometric Unwarping Transformer using Doc3D dataset for accurate geometric unwrapping.</li>
    <li>Used DocProj dataset for Illumination Correction Transformer to resolve illumination problems during document scanning.</li>
  </ul>
</td>
</tr></table>

<!-- Sky Changer -->
<table class="imgtable"><tr>
<td style="width:180px; vertical-align:top;">
  <img src="./Figures/skychange.gif" alt="Project GIF" width="500px" height="300px" />
</td>
<td align="left">
  <b>Sky Changer</b><br />
  <i>Python, PyTorch, UNET, Fast API, Colab, VS code</i>
  <ul>
    <li>Developed a model with sky-changing (Multiband blender), high-res processing (UNET), and low-res processing (hrnet-ocr).</li>
    <li>Enhanced segmentation in the low-res module using ASPP and improved high-res module with learnable parameters.</li>
    <li>Used Multiband blender for effective sky replacement.</li>
  </ul>
</td>
</tr></table>

<!-- Text-to Image Generator -->
<table class="imgtable"><tr>
<td style="width:180px; vertical-align:top;">
  <img src="./Figures/Aiart.gif" alt="Project GIF" width="500px" height="300px" />
</td>
<td align="left">
  <b>Text-to Image Generator</b><br />
  <i>Python, PyTorch, Hugging Face, Stable Diffusion, Fast API, Colab, VS code</i>
  <ul>
    <li>Implemented Stable Diffusion for text-to-image generation, using fine-tuning and Dreambooth for high-quality, customized images.</li>
  </ul>
</td>
</tr></table>

<!-- Face Swap -->
<table class="imgtable"><tr>
<td style="width:180px; vertical-align:top;">
  <img src="./Figures/faceswap.png" alt="Project GIF" width="500px" height="300px" />
</td>
<td align="left">
  <b>Face Swap</b><br />
  <i>Python, Dlib, PyTorch, face-recognition, TensorFlow, Fast API, Colab, VS code</i>
  <ul>
    <li>Implemented pose-matching and facial point detection for accurate face swapping using Dlib.</li>
    <li>Blended faces to ensure natural skin color transitions.</li>
  </ul>
</td>
</tr></table>

</td></tr></table>


<div class="infoblock">
<div class="blockcontent">
<h2>Achievements</h2>
<ul>
<li>Awarded a fully funded scholarship for my Master's at the prestigious LIESMARS Lab, Wuhan University—renowned globally for its pioneering research in photogrammetry, remote sensing, and geospatial information science.</li>  
<li>Ranked 2nd with the ‘Weed Detection System’ in BS Computer Science Final Year Project.</li>
<li>Won 2nd place at the 2023 Hackathon with the ‘Weed Detection System’.</li>
</ul>
</div>
</div>

<div id="footer">
<div id="footer-text">
Page updated 2025-07-29</a>.
</div>
</div>
</div>
</body>
</html>